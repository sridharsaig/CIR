{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIR",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1u0TNt3ah5l83e1hr-u89Nn38RMNBgQ2E",
      "authorship_tag": "ABX9TyMLoqjZgMKOPZNgk0u5kJ5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridharsaig/CIR/blob/master/CIR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBuFwP-G3jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1EkJY4qHvY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "import imageio\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from skimage import img_as_ubyte\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "\n",
        "# io utils\n",
        "from pytorch3d.io import load_obj\n",
        "\n",
        "# datastructures\n",
        "from pytorch3d.structures import Meshes, Textures\n",
        "\n",
        "# 3D transformations functions\n",
        "from pytorch3d.transforms import Rotate, Translate\n",
        "\n",
        "# rendering components\n",
        "from pytorch3d.renderer import (\n",
        "    OpenGLPerspectiveCameras, look_at_view_transform, look_at_rotation, \n",
        "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
        "    SoftSilhouetteShader, HardPhongShader, PointLights\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUTAmyggv0SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the cuda device \n",
        "device = torch.device(\"cuda:0\")\n",
        "torch.cuda.set_device(device)\n",
        "\n",
        "# Load the obj and ignore the textures and materials.\n",
        "objInput = input(\"Enter the Folder Path of the 3D File : \")\n",
        "verts, faces_idx, _ = load_obj(objInput)\n",
        "faces = faces_idx.verts_idx\n",
        "\n",
        "# Load the 2D scene silhouette of the image\n",
        "silInput = input(\"Enter the Folder Path of the Silhouette Image : \")\n",
        "silImage = Image.open(silInput)\n",
        "matplotSil = image.imread(silInput, \"r\")\n",
        "arrData = asarray(silImage)\n",
        "#print(silData.shape)\n",
        "\n",
        "# Initialize each vertex to be white in color.\n",
        "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
        "textures = Textures(verts_rgb=verts_rgb.to(device))\n",
        "\n",
        "# Create a Meshes object for the teapot. Here we have only one mesh in the batch.\n",
        "teapot_mesh = Meshes(\n",
        "    verts=[verts.to(device)],   \n",
        "    faces=[faces.to(device)], \n",
        "    textures=textures\n",
        ")\n",
        "\n",
        "bwImage = silImage.convert('1')\n",
        "bw_image = bwImage.convert('RGBA')\n",
        "bwData = asarray(bw_image)\n",
        "\n",
        "print(bwData)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(bwData.squeeze())\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7q2n_LyMVfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an OpenGL perspective camera.\n",
        "cameras = OpenGLPerspectiveCameras(device=device)\n",
        "\n",
        "# To blend the 100 faces we set a few parameters which control the opacity and the sharpness of \n",
        "# edges. Refer to blending.py for more details. \n",
        "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
        "\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 256x256. To form the blended image we use 100 faces for each pixel. We also set bin_size and max_faces_per_bin to None which ensure that \n",
        "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
        "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
        "# the difference between naive and coarse-to-fine rasterization. \n",
        "raster_settings = RasterizationSettings(\n",
        "    image_size=256, \n",
        "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma, \n",
        "    faces_per_pixel=100, \n",
        ")\n",
        "\n",
        "# Create a silhouette mesh renderer by composing a rasterizer and a shader. \n",
        "silhouette_renderer = MeshRenderer(\n",
        "    rasterizer=MeshRasterizer(\n",
        "        cameras=cameras, \n",
        "        raster_settings=raster_settings\n",
        "    ),\n",
        "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
        ")\n",
        "\n",
        "# We will also create a phong renderer. This is simpler and only needs to render one face per pixel.\n",
        "raster_settings = RasterizationSettings(\n",
        "    image_size=256, \n",
        "    blur_radius=0.0, \n",
        "    faces_per_pixel=1, \n",
        ")\n",
        "\n",
        "# We can add a point light in front of the object. \n",
        "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
        "phong_renderer = MeshRenderer(\n",
        "    rasterizer=MeshRasterizer(\n",
        "        cameras=cameras, \n",
        "        raster_settings=raster_settings\n",
        "    ),\n",
        "    shader=HardPhongShader(device=device, lights=lights)\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo-GDhSk5nQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select the viewpoint using spherical angles  \n",
        "distance = 10   # distance from camera to the object\n",
        "elevation = 100.0   # angle of elevation in degrees\n",
        "azimuth = 0.0  # No rotation so the camera is positioned on the +Z axis. \n",
        "\n",
        "# Get the position of the camera based on the spherical angles\n",
        "R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
        "\n",
        "# Render the teapot providing the values of R and T. \n",
        "silhouete = silhouette_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
        "image_ref = phong_renderer(meshes_world=teapot_mesh, R=R, T=T)\n",
        "\n",
        "silhouete = silhouete.cpu().numpy()\n",
        "image_ref = image_ref.cpu().numpy()\n",
        "\n",
        "#plt.imsave('bpin.jpeg', silhouete.squeeze()[..., 3])\n",
        "#plt.imsave('bpin_real.jpeg', image_ref.squeeze())\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(silhouete.squeeze()[..., 3])  # only plot the alpha channel of the RGBA image\n",
        "plt.grid(False)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image_ref.squeeze())\n",
        "plt.grid(False)\n",
        "\n",
        "#print(silhouete)\n",
        "#print(image_ref)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqANWBnC7fOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, meshes, renderer, image_ref):\n",
        "        super().__init__()\n",
        "        self.meshes = meshes\n",
        "        self.device = meshes.device\n",
        "        self.renderer = renderer\n",
        "        \n",
        "        # Get the silhouette of the reference RGB image by finding all the non zero values. \n",
        "        #image_ref = torch.from_numpy(bwData)\n",
        "        image_ref = torch.from_numpy((bwData[..., :3].max(-1) != 0).astype(np.float32))\n",
        "        self.register_buffer('image_ref', image_ref)\n",
        "\n",
        "        # Create an optimizable parameter for the x, y, z position of the camera. \n",
        "        self.camera_position = nn.Parameter(\n",
        "            torch.from_numpy(np.array([3.0,  6.9, +2.5], dtype=np.float32)).to(meshes.device))\n",
        "\n",
        "    def forward(self):\n",
        "        \n",
        "        # Render the image using the updated camera position. Based on the new position of the \n",
        "        # camera we calculate the rotation and translation matrices\n",
        "        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n",
        "        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]   # (1, 3)\n",
        "        \n",
        "        image = self.renderer(meshes_world=self.meshes.clone(), R=R, T=T)\n",
        "        \n",
        "        # Calculate the silhouette loss\n",
        "        loss = torch.sum((image[...,3] - self.image_ref) ** 2)\n",
        "        return loss, image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8uUbvSo-HZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will save images periodically and compose them into a GIF.\n",
        "filename_output = \"./Output.gif\"\n",
        "writer = imageio.get_writer(filename_output, mode='I', duration=0.3)\n",
        "\n",
        "# Initialize a model using the renderer, mesh and reference image\n",
        "model = Model(meshes=teapot_mesh, renderer=silhouette_renderer, image_ref=image_ref).to(device)\n",
        "print(model.image_ref)\n",
        "#print(image_init)\n",
        "# Create an optimizer. Here we are using Adam and we pass in the parameters of the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqI1qBg8-QNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "_, image_init = model()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_init.detach().squeeze().cpu().numpy()[..., 3])\n",
        "plt.grid(False)\n",
        "plt.title(\"Starting position\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(model.image_ref.cpu().numpy().squeeze())\n",
        "plt.grid(False)\n",
        "plt.title(\"Reference silhouette\")\n",
        "\n",
        "print(image_init)\n",
        "print(image_ref)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQHMIeum_Gh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loop = tqdm_notebook(range(200))\n",
        "for i in loop:\n",
        "    optimizer.zero_grad()\n",
        "    loss, _ = model()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    loop.set_description('Optimizing (loss %.4f)' % loss.data)\n",
        "    \n",
        "    if loss.item() < 200:\n",
        "        break\n",
        "    \n",
        "    # Save outputs to create a GIF. \n",
        "    if i % 10 == 0:\n",
        "        R = look_at_rotation(model.camera_position[None, :], device=model.device)\n",
        "        T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]   # (1, 3)\n",
        "        image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
        "        image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
        "        image = img_as_ubyte(image)\n",
        "        writer.append_data(image)\n",
        "        \n",
        "        print(model.camera_position)\n",
        "        plt.figure()\n",
        "        plt.imshow(image[..., :3])\n",
        "        plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
        "        plt.grid(\"off\")\n",
        "        plt.axis(\"off\")\n",
        "    \n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bKjFBI6SYC8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}